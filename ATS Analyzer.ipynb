{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKX2bGuqyJatniGnNLoCbe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stdioch1/Machine-Learning/blob/main/ATS%20Analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W4G2O_kmlEP4",
        "outputId": "38c826a9-44a5-4d21-db25-45ad4f871ff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-md==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.1)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.12.2)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Collecting textract\n",
            "  Using cached textract-1.6.5-py3-none-any.whl.metadata (2.5 kB)\n",
            "\u001b[33mWARNING: Ignoring version 1.6.5 of textract since it has invalid metadata:\n",
            "Requested textract from https://files.pythonhosted.org/packages/6b/3e/ac16b6bf28edf78296aea7d0cb416b49ed30282ac8c711662541015ee6f3/textract-1.6.5-py3-none-any.whl has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    extract-msg (<=0.29.*)\n",
            "                 ~~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Using cached textract-1.6.4.tar.gz (17 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "FileUpload imported from ipywidgets.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PackageNotFoundError",
          "evalue": "Package not found at 'SATHISH RESUME.docx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-10add9849d30>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-10add9849d30>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;31m# Parse the resume content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m     \u001b[0mresume_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Analyze against all ATS systems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-10add9849d30>\u001b[0m in \u001b[0;36mparse_resume\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mfile_extension\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'.docx'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_docx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mfile_extension\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'.txt'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-10add9849d30>\u001b[0m in \u001b[0;36m_parse_docx\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_parse_docx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;34m\"\"\"Extract text from DOCX file\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpara\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparagraphs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/docx/api.py\u001b[0m in \u001b[0;36mDocument\u001b[0;34m(docx)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \"\"\"\n\u001b[1;32m     26\u001b[0m     \u001b[0mdocx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default_docx_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdocx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdocx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mdocument_part\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DocumentPart\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPackage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_document_part\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdocument_part\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent_type\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWML_DOCUMENT_MAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtmpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"file '%s' is not a Word file, content type is '%s'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/docx/opc/package.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, pkg_file)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkg_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOpcPackage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;34m\"\"\"Return an |OpcPackage| instance loaded with the contents of `pkg_file`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mpkg_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackageReader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mpackage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mUnmarshaller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munmarshal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPartFactory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/docx/opc/pkgreader.py\u001b[0m in \u001b[0;36mfrom_file\u001b[0;34m(pkg_file)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;34m\"\"\"Return a |PackageReader| instance loaded with contents of `pkg_file`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mphys_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhysPkgReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mcontent_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ContentTypeMap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_xml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphys_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent_types_xml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mpkg_srels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackageReader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_srels_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphys_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPACKAGE_URI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/docx/opc/phys_pkg.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, pkg_file)\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mreader_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ZipPkgReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mPackageNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Package not found at '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpkg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# assume it's a stream and pass it to Zip reader to sort out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mreader_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ZipPkgReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPackageNotFoundError\u001b[0m: Package not found at 'SATHISH RESUME.docx'"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_md\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# %%\n",
        "!pip install python-docx # Install the python-docx package to read DOCX files.\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "!pip install PyPDF2\n",
        "import docx\n",
        "import json\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import spacy\n",
        "!pip install textract\n",
        "!pip install requests\n",
        "import requests # Import the requests library to make HTTP requests\n",
        "try:\n",
        "    from IPython.display import display, FileUpload # Import FileUpload for resume upload\n",
        "except ImportError:\n",
        "    # If still not found, try importing from ipywidgets\n",
        "    from ipywidgets import FileUpload\n",
        "    print(\"FileUpload imported from ipywidgets.\")\n",
        "\n",
        "class ATSAnalyzer:\n",
        "    def __init__(self):\n",
        "        # Initialize NLP tools\n",
        "        nltk.download('punkt')\n",
        "        nltk.download('stopwords')\n",
        "        self.nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "        # Define ATS systems and their specific algorithms/configurations\n",
        "        self.ats_systems = {\n",
        "            'workday': {\n",
        "                'parser_type': 'keyword_focused',\n",
        "                'section_importance': {'skills': 0.3, 'experience': 0.5, 'education': 0.2},\n",
        "                'format_preference': 'chronological',\n",
        "                'keyword_match_weight': 0.7\n",
        "            },\n",
        "            'icims': {\n",
        "                'parser_type': 'context_aware',\n",
        "                'section_importance': {'skills': 0.25, 'experience': 0.55, 'education': 0.2},\n",
        "                'format_preference': 'chronological',\n",
        "                'keyword_match_weight': 0.6\n",
        "            },\n",
        "            'bamboohr': {\n",
        "                'parser_type': 'semantic',\n",
        "                'section_importance': {'skills': 0.35, 'experience': 0.45, 'education': 0.2},\n",
        "                'format_preference': 'flexible',\n",
        "                'keyword_match_weight': 0.5\n",
        "            },\n",
        "            'greenhouse': {\n",
        "                'parser_type': 'hybrid',\n",
        "                'section_importance': {'skills': 0.4, 'experience': 0.4, 'education': 0.2},\n",
        "                'format_preference': 'flexible',\n",
        "                'keyword_match_weight': 0.55\n",
        "            },\n",
        "            'taleo': {\n",
        "                'parser_type': 'keyword_focused',\n",
        "                'section_importance': {'skills': 0.3, 'experience': 0.5, 'education': 0.2},\n",
        "                'format_preference': 'strict_chronological',\n",
        "                'keyword_match_weight': 0.75\n",
        "            },\n",
        "            'lever': {\n",
        "                'parser_type': 'semantic',\n",
        "                'section_importance': {'skills': 0.4, 'experience': 0.4, 'education': 0.2},\n",
        "                'format_preference': 'flexible',\n",
        "                'keyword_match_weight': 0.5\n",
        "            },\n",
        "            'successfactors': {\n",
        "                'parser_type': 'hybrid',\n",
        "                'section_importance': {'skills': 0.3, 'experience': 0.5, 'education': 0.2},\n",
        "                'format_preference': 'chronological',\n",
        "                'keyword_match_weight': 0.65\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def parse_resume(self, file_path):\n",
        "        \"\"\"Extract text from resume file (PDF, DOCX, or TXT)\"\"\"\n",
        "        _, file_extension = os.path.splitext(file_path)\n",
        "\n",
        "        if file_extension.lower() == '.pdf':\n",
        "            text = self._parse_pdf(file_path)\n",
        "        elif file_extension.lower() == '.docx':\n",
        "            text = self._parse_docx(file_path)\n",
        "        elif file_extension.lower() == '.txt':\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                text = file.read()\n",
        "        else:\n",
        "            # Attempt to use textract for other file types\n",
        "            try:\n",
        "                text = textract.process(file_path).decode('utf-8')\n",
        "            except:\n",
        "                raise ValueError(f\"Unsupported file format: {file_extension}\")\n",
        "\n",
        "        return text\n",
        "\n",
        "    def _parse_pdf(self, file_path):\n",
        "        \"\"\"Extract text from PDF file\"\"\"\n",
        "        text = \"\"\n",
        "        with open(file_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            for page in reader.pages:\n",
        "                text += page.extract_text() + \"\\n\"\n",
        "        return text\n",
        "\n",
        "    def _parse_docx(self, file_path):\n",
        "        \"\"\"Extract text from DOCX file\"\"\"\n",
        "        doc = docx.Document(file_path)\n",
        "        text = \"\"\n",
        "        for para in doc.paragraphs:\n",
        "            text += para.text + \"\\n\"\n",
        "        return text\n",
        "\n",
        "    def parse_job_description(self, jd_text):\n",
        "        \"\"\"Parse and extract key information from job description\"\"\"\n",
        "        doc = self.nlp(jd_text)\n",
        "\n",
        "        # Extract requirements, skills, and qualifications\n",
        "        skills = []\n",
        "        requirements = []\n",
        "        qualifications = []\n",
        "\n",
        "        # Simple keyword-based extraction (in a real implementation,\n",
        "        # this would use more sophisticated NLP techniques)\n",
        "        skill_indicators = [\"proficient in\", \"experience with\", \"knowledge of\", \"skills\", \"ability to\"]\n",
        "        req_indicators = [\"requirements\", \"required\", \"must have\", \"essential\"]\n",
        "        qual_indicators = [\"qualifications\", \"education\", \"degree\", \"certification\"]\n",
        "\n",
        "        for sentence in doc.sents:\n",
        "            sent_text = sentence.text.lower()\n",
        "\n",
        "            # Check for skills\n",
        "            if any(indicator in sent_text for indicator in skill_indicators):\n",
        "                skills.append(sentence.text)\n",
        "\n",
        "            # Check for requirements\n",
        "            if any(indicator in sent_text for indicator in req_indicators):\n",
        "                requirements.append(sentence.text)\n",
        "\n",
        "            # Check for qualifications\n",
        "            if any(indicator in sent_text for indicator in qual_indicators):\n",
        "                qualifications.append(sentence.text)\n",
        "\n",
        "        # Extract important keywords using NLP\n",
        "        keywords = []\n",
        "        for token in doc:\n",
        "            if (token.is_alpha and not token.is_stop and\n",
        "                (token.pos_ in [\"NOUN\", \"PROPN\", \"ADJ\"] or token.ent_type_)):\n",
        "                keywords.append(token.text.lower())\n",
        "\n",
        "        # Count frequency of keywords\n",
        "        keyword_freq = {}\n",
        "        for word in keywords:\n",
        "            if word in keyword_freq:\n",
        "                keyword_freq[word] += 1\n",
        "            else:\n",
        "                keyword_freq[word] = 1\n",
        "\n",
        "        # Sort by frequency\n",
        "        sorted_keywords = sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)\n",
        "        top_keywords = [k for k, v in sorted_keywords[:30]]  # Top 30 keywords\n",
        "\n",
        "        return {\n",
        "            \"skills\": skills,\n",
        "            \"requirements\": requirements,\n",
        "            \"qualifications\": qualifications,\n",
        "            \"keywords\": top_keywords\n",
        "        }\n",
        "\n",
        "    def segment_resume(self, resume_text):\n",
        "        \"\"\"Split resume into sections like experience, education, skills\"\"\"\n",
        "        # This is a simplified approach - a production system would use more robust section detection\n",
        "        sections = {}\n",
        "\n",
        "        # Common section headers\n",
        "        section_headers = {\n",
        "            \"experience\": [\"experience\", \"work experience\", \"professional experience\", \"employment\"],\n",
        "            \"education\": [\"education\", \"academic background\", \"academic\", \"educational background\"],\n",
        "            \"skills\": [\"skills\", \"technical skills\", \"core competencies\", \"key skills\", \"capabilities\"],\n",
        "            \"summary\": [\"summary\", \"professional summary\", \"profile\", \"about me\"],\n",
        "            \"certifications\": [\"certifications\", \"certificates\", \"credentials\"],\n",
        "            \"projects\": [\"projects\", \"personal projects\", \"professional projects\"]\n",
        "        }\n",
        "\n",
        "        # Find potential section boundaries\n",
        "        lines = resume_text.split('\\n')\n",
        "        current_section = \"unknown\"\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            clean_line = line.strip().lower()\n",
        "\n",
        "            # Check if this line is a section header\n",
        "            for section, headers in section_headers.items():\n",
        "                if any(header == clean_line or header in clean_line for header in headers):\n",
        "                    current_section = section\n",
        "                    sections[current_section] = []\n",
        "                    break\n",
        "\n",
        "            # Add content to current section\n",
        "            if current_section in sections:\n",
        "                sections[current_section].append(line)\n",
        "\n",
        "        # Join the content within each section\n",
        "        for section in sections:\n",
        "            sections[section] = '\\n'.join(sections[section])\n",
        "\n",
        "        return sections\n",
        "\n",
        "    def extract_entities(self, text):\n",
        "        \"\"\"Extract named entities (companies, titles, dates, etc.)\"\"\"\n",
        "        doc = self.nlp(text)\n",
        "        entities = {}\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ not in entities:\n",
        "                entities[ent.label_] = []\n",
        "            entities[ent.label_].append(ent.text)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def analyze_format(self, resume_text):\n",
        "        \"\"\"Analyze the format and structure of the resume\"\"\"\n",
        "        lines = resume_text.split('\\n')\n",
        "        structure = {\n",
        "            \"total_lines\": len(lines),\n",
        "            \"blank_lines\": sum(1 for line in lines if not line.strip()),\n",
        "            \"bullet_points\": sum(1 for line in lines if line.strip().startswith(('•', '-', '*', '→'))),\n",
        "            \"avg_line_length\": sum(len(line) for line in lines if line.strip()) / max(1, sum(1 for line in lines if line.strip())),\n",
        "            \"sections\": len(self.segment_resume(resume_text))\n",
        "        }\n",
        "\n",
        "        # Determine format type\n",
        "        if structure[\"bullet_points\"] > structure[\"total_lines\"] * 0.3:\n",
        "            format_type = \"bullet-heavy\"\n",
        "        else:\n",
        "            format_type = \"paragraph-style\"\n",
        "\n",
        "        # Check for chronological vs functional\n",
        "        resume_sections = self.segment_resume(resume_text)\n",
        "        if \"experience\" in resume_sections:\n",
        "            exp_text = resume_sections[\"experience\"]\n",
        "            date_pattern = r'(19|20)\\d{2}\\s*(-|–|to|—)\\s*(present|current|now|[0-9]{4})'\n",
        "            dates = re.findall(date_pattern, exp_text, re.IGNORECASE)\n",
        "\n",
        "            if len(dates) >= 2:\n",
        "                # Check if dates are in descending order (newest first)\n",
        "                is_chronological = True\n",
        "                for i in range(len(dates) - 1):\n",
        "                    # This is simplified logic - would need more robust date parsing\n",
        "                    if \"present\" in dates[i][2].lower() or \"current\" in dates[i][2].lower() or \"now\" in dates[i][2].lower():\n",
        "                        continue\n",
        "\n",
        "                    if dates[i][0] < dates[i+1][0]:\n",
        "                        is_chronological = False\n",
        "\n",
        "                format_type += \", chronological\" if is_chronological else \", non-chronological\"\n",
        "            else:\n",
        "                format_type += \", possibly functional\"\n",
        "\n",
        "        return {\n",
        "            \"format_type\": format_type,\n",
        "            \"structure\": structure\n",
        "        }\n",
        "\n",
        "    def calculate_keyword_match(self, resume_text, job_keywords):\n",
        "        \"\"\"Calculate the keyword match percentage\"\"\"\n",
        "        resume_text = resume_text.lower()\n",
        "        matched_keywords = [k for k in job_keywords if k.lower() in resume_text]\n",
        "\n",
        "        match_percentage = len(matched_keywords) / len(job_keywords) * 100\n",
        "        return {\n",
        "            \"match_percentage\": match_percentage,\n",
        "            \"matched_keywords\": matched_keywords,\n",
        "            \"missing_keywords\": [k for k in job_keywords if k.lower() not in resume_text]\n",
        "        }\n",
        "\n",
        "    def analyze_ats_compatibility(self, resume_text, job_description, ats_name=\"all\"):\n",
        "        \"\"\"Analyze resume compatibility with specific or all ATS systems\"\"\"\n",
        "        # Parse job description\n",
        "        jd_data = self.parse_job_description(job_description)\n",
        "\n",
        "        # Segment resume\n",
        "        resume_sections = self.segment_resume(resume_text)\n",
        "\n",
        "        # Format analysis\n",
        "        format_analysis = self.analyze_format(resume_text)\n",
        "\n",
        "        # Keyword match\n",
        "        keyword_match = self.calculate_keyword_match(resume_text, jd_data[\"keywords\"])\n",
        "\n",
        "        # Specific ATS systems to analyze\n",
        "        systems_to_analyze = []\n",
        "        if ats_name.lower() == \"all\":\n",
        "            systems_to_analyze = list(self.ats_systems.keys())\n",
        "        elif ats_name.lower() in self.ats_systems:\n",
        "            systems_to_analyze = [ats_name.lower()]\n",
        "        else:\n",
        "            return {\"error\": f\"ATS system '{ats_name}' not recognized\"}\n",
        "\n",
        "        # Analyze for each ATS\n",
        "        results = {}\n",
        "        for system in systems_to_analyze:\n",
        "            ats_config = self.ats_systems[system]\n",
        "\n",
        "            # Calculate base score from keyword match\n",
        "            keyword_score = keyword_match[\"match_percentage\"] / 100 * ats_config[\"keyword_match_weight\"]\n",
        "\n",
        "            # Format compatibility score\n",
        "            format_score = 0\n",
        "            if ats_config[\"format_preference\"] == \"chronological\" and \"chronological\" in format_analysis[\"format_type\"]:\n",
        "                format_score = 0.2\n",
        "            elif ats_config[\"format_preference\"] == \"flexible\":\n",
        "                format_score = 0.15\n",
        "            elif ats_config[\"format_preference\"] == \"strict_chronological\" and \"chronological\" in format_analysis[\"format_type\"]:\n",
        "                format_score = 0.25\n",
        "\n",
        "            # Section weighting\n",
        "            section_score = 0\n",
        "            for section, weight in ats_config[\"section_importance\"].items():\n",
        "                if section in resume_sections:\n",
        "                    # Simplified scoring - in a real implementation, would analyze section content\n",
        "                    section_keywords = [k for k in jd_data[\"keywords\"] if k.lower() in resume_sections[section].lower()]\n",
        "                    section_match = len(section_keywords) / max(1, len(jd_data[\"keywords\"]))\n",
        "                    section_score += section_match * weight\n",
        "\n",
        "            # Calculate final score for this ATS\n",
        "            final_score = (keyword_score + format_score + section_score) * 100\n",
        "\n",
        "            # Cap at 100%\n",
        "            final_score = min(100, final_score)\n",
        "\n",
        "            # Generate recommendations\n",
        "            recommendations = []\n",
        "\n",
        "            # Keyword recommendations\n",
        "            if keyword_match[\"match_percentage\"] < 70:\n",
        "                recommendations.append(f\"Add more of these missing keywords: {', '.join(keyword_match['missing_keywords'][:5])}\")\n",
        "\n",
        "            # Format recommendations\n",
        "            if ats_config[\"format_preference\"] == \"strict_chronological\" and \"chronological\" not in format_analysis[\"format_type\"]:\n",
        "                recommendations.append(\"Use a strict chronological format with clear date ranges\")\n",
        "\n",
        "            # Section recommendations\n",
        "            for section, weight in ats_config[\"section_importance\"].items():\n",
        "                if section not in resume_sections and weight > 0.1:\n",
        "                    recommendations.append(f\"Add a dedicated '{section}' section\")\n",
        "\n",
        "            results[system] = {\n",
        "                \"score\": round(final_score, 1),\n",
        "                \"strengths\": {\n",
        "                    \"keyword_match\": keyword_match[\"match_percentage\"],\n",
        "                    \"format_compatibility\": \"High\" if format_score > 0.15 else \"Medium\" if format_score > 0 else \"Low\",\n",
        "                    \"section_coverage\": round(section_score * 100 / sum(ats_config[\"section_importance\"].values()), 1)\n",
        "                },\n",
        "                \"recommendations\": recommendations\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            \"overall_compatibility\": round(sum(r[\"score\"] for r in results.values()) / len(results), 1),\n",
        "            \"keyword_match\": keyword_match,\n",
        "            \"format_analysis\": format_analysis,\n",
        "            \"ats_specific_results\": results\n",
        "        }\n",
        "\n",
        "    def generate_report(self, analysis_results, output_format=\"json\"):\n",
        "        \"\"\"Generate a comprehensive report of the analysis\"\"\"\n",
        "        if output_format == \"json\":\n",
        "            return json.dumps(analysis_results, indent=2)\n",
        "        elif output_format == \"text\":\n",
        "            # Create plain text report\n",
        "            report = []\n",
        "            report.append(\"RESUME ATS COMPATIBILITY ANALYSIS\")\n",
        "            report.append(\"=\" * 50)\n",
        "            report.append(f\"Overall Compatibility Score: {analysis_results['overall_compatibility']}%\")\n",
        "            report.append(\"-\" * 50)\n",
        "\n",
        "            report.append(\"KEYWORD ANALYSIS\")\n",
        "            report.append(f\"Match Percentage: {analysis_results['keyword_match']['match_percentage']:.1f}%\")\n",
        "            report.append(\"Top Matched Keywords:\")\n",
        "            for kw in analysis_results['keyword_match']['matched_keywords'][:10]:\n",
        "                report.append(f\"  - {kw}\")\n",
        "            report.append(\"Missing Keywords:\")\n",
        "            for kw in analysis_results['keyword_match']['missing_keywords'][:10]:\n",
        "                report.append(f\"  - {kw}\")\n",
        "\n",
        "            report.append(\"-\" * 50)\n",
        "            report.append(\"FORMAT ANALYSIS\")\n",
        "            report.append(f\"Format Type: {analysis_results['format_analysis']['format_type']}\")\n",
        "\n",
        "            report.append(\"-\" * 50)\n",
        "            report.append(\"ATS-SPECIFIC RESULTS\")\n",
        "\n",
        "            for ats, results in analysis_results['ats_specific_results'].items():\n",
        "                report.append(f\"\\n{ats.upper()} - Score: {results['score']}%\")\n",
        "                report.append(\"Strengths:\")\n",
        "                for key, value in results['strengths'].items():\n",
        "                    report.append(f\"  - {key}: {value}\")\n",
        "\n",
        "                report.append(\"Recommendations:\")\n",
        "                for rec in results['recommendations']:\n",
        "                    report.append(f\"  - {rec}\")\n",
        "\n",
        "            return \"\\n\".join(report)\n",
        "        else:\n",
        "            return {\"error\": f\"Output format '{output_format}' not supported\"}\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "def main():\n",
        "    analyzer = ATSAnalyzer()\n",
        "\n",
        "    # Fetch job description from URL\n",
        "    job_description_url = \"https://careers-vizio.icims.com/jobs/3432/senior%2c-data-scientist/job?mode=view&mobile=false&width=1170&height=500&bga=true&needsRedirect=false&jan1offset=-360&jun1offset=-300\"\n",
        "    response = requests.get(job_description_url)\n",
        "    job_description = response.text  # Assuming the response is HTML, you might need to further extract the relevant job description text using an HTML parser.\n",
        "\n",
        "    # Provide the path to your resume file directly\n",
        "    resume_path = \"SATHISH RESUME.docx\"  # Update with the correct path if necessary\n",
        "\n",
        "    # Parse the resume content\n",
        "    resume_text = analyzer.parse_resume(resume_path)\n",
        "\n",
        "    # Analyze against all ATS systems\n",
        "    analysis_results = analyzer.analyze_ats_compatibility(resume_text, job_description, \"all\")\n",
        "\n",
        "    # Generate and print report\n",
        "    report = analyzer.generate_report(analysis_results, \"text\")\n",
        "    print(report)\n",
        "\n",
        "    # Save JSON report\n",
        "    with open(\"ats_analysis_report.json\", \"w\") as f:\n",
        "        json.dump(analysis_results, f, indent=2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}